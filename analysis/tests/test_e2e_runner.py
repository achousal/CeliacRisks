"""
End-to-end runner tests for user-critical flows.

Tests the full pipeline workflows with realistic small fixtures:
1. Full pipeline: splits -> train -> aggregate
2. Ensemble workflow: base models -> ensemble -> aggregate
3. HPC workflow: config validation -> dry-run
4. Temporal validation: temporal splits -> train -> evaluate

These tests verify integration between components with deterministic fixtures.
Run with: pytest tests/test_e2e_runner.py -v
Run slow tests: pytest tests/test_e2e_runner.py -v -m slow
"""

import json

import numpy as np
import pandas as pd
import pytest
import yaml
from ced_ml.cli.main import cli
from ced_ml.data.schema import CONTROL_LABEL, ID_COL, INCIDENT_LABEL, PREVALENT_LABEL, TARGET_COL
from click.testing import CliRunner

# ==================== Fixtures ====================


@pytest.fixture
def minimal_proteomics_data(tmp_path):
    """
    Create minimal proteomics dataset for E2E testing.

    200 samples: 150 controls, 30 incident, 20 prevalent
    15 protein features + demographics
    Small enough for fast tests but realistic structure.
    """
    rng = np.random.default_rng(42)

    n_controls = 150
    n_incident = 30
    n_prevalent = 20
    n_total = n_controls + n_incident + n_prevalent
    n_proteins = 15

    # Create labels
    labels = (
        [CONTROL_LABEL] * n_controls
        + [INCIDENT_LABEL] * n_incident
        + [PREVALENT_LABEL] * n_prevalent
    )

    # Create base data
    data = {
        ID_COL: [f"SAMPLE_{i:04d}" for i in range(n_total)],
        TARGET_COL: labels,
        "age": rng.integers(25, 75, n_total),
        "BMI": rng.uniform(18, 35, n_total),
        "sex": rng.choice(["M", "F"], n_total),
        "Genetic ethnic grouping": rng.choice(["White", "Asian"], n_total),
    }

    # Add protein columns with realistic signal
    # Incident cases have higher values for some proteins
    for i in range(n_proteins):
        base = rng.standard_normal(n_total)
        signal = np.zeros(n_total)

        # Add signal for cases (incident stronger than prevalent)
        if i < 5:  # First 5 proteins have signal
            signal[n_controls : n_controls + n_incident] = rng.normal(1.0, 0.3, n_incident)
            signal[n_controls + n_incident :] = rng.normal(0.8, 0.3, n_prevalent)

        data[f"PROT_{i:03d}_resid"] = base + signal

    df = pd.DataFrame(data)

    # Save as parquet
    parquet_path = tmp_path / "minimal_proteomics.parquet"
    df.to_parquet(parquet_path, index=False)

    return parquet_path


@pytest.fixture
def temporal_proteomics_data(tmp_path):
    """
    Create proteomics dataset with temporal component for temporal validation testing.

    200 samples with sample_date spanning 2020-2023
    Temporal split should use chronological ordering.
    """
    rng = np.random.default_rng(42)

    n_controls = 150
    n_incident = 30
    n_prevalent = 20
    n_total = n_controls + n_incident + n_prevalent
    n_proteins = 15

    labels = (
        [CONTROL_LABEL] * n_controls
        + [INCIDENT_LABEL] * n_incident
        + [PREVALENT_LABEL] * n_prevalent
    )

    # Generate dates spanning 2020-2023 (chronologically ordered)
    base_date = pd.Timestamp("2020-01-01")
    days_span = 1460  # ~4 years
    dates = [base_date + pd.Timedelta(days=int(d)) for d in np.linspace(0, days_span, n_total)]

    data = {
        ID_COL: [f"SAMPLE_{i:04d}" for i in range(n_total)],
        TARGET_COL: labels,
        "sample_date": dates,
        "age": rng.integers(25, 75, n_total),
        "BMI": rng.uniform(18, 35, n_total),
        "sex": rng.choice(["M", "F"], n_total),
        "Genetic ethnic grouping": rng.choice(["White", "Asian"], n_total),
    }

    # Add proteins
    for i in range(n_proteins):
        base = rng.standard_normal(n_total)
        signal = np.zeros(n_total)

        if i < 5:
            signal[n_controls : n_controls + n_incident] = rng.normal(1.0, 0.3, n_incident)
            signal[n_controls + n_incident :] = rng.normal(0.8, 0.3, n_prevalent)

        data[f"PROT_{i:03d}_resid"] = base + signal

    df = pd.DataFrame(data)

    parquet_path = tmp_path / "temporal_proteomics.parquet"
    df.to_parquet(parquet_path, index=False)

    return parquet_path


@pytest.fixture
def minimal_training_config(tmp_path):
    """
    Create minimal training config for fast E2E tests.

    Reduced CV folds and iterations for speed.
    """
    config = {
        "scenario": "IncidentOnly",
        "cv": {
            "folds": 2,
            "repeats": 1,
            "inner_folds": 2,
            "scoring": "roc_auc",
            "n_jobs": 1,
            "random_state": 42,
        },
        "optuna": {
            "enabled": False,  # Disable for speed
        },
        "features": {
            "feature_select": "hybrid",
            "kbest_scope": "protein",
            "screen_method": "mannwhitney",
            "screen_top_n": 10,
            "k_grid": [3, 5],
            "stability_thresh": 0.7,
            "corr_thresh": 0.85,
        },
        "calibration": {
            "enabled": True,
            "method": "isotonic",
            "strategy": "oof_posthoc",
        },
        "thresholds": {
            "objective": "youden",
            "fixed_spec": 0.95,
        },
        "lr": {
            "C_min": 0.1,
            "C_max": 10.0,
            "C_points": 2,
            "l1_ratio": [0.5],
            "solver": "saga",
            "max_iter": 500,
        },
        "rf": {
            "n_estimators_grid": [50],
            "max_depth_grid": [3],
            "min_samples_split_grid": [2],
            "min_samples_leaf_grid": [1],
            "max_features_grid": [0.5],
        },
        "xgboost": {
            "n_estimators_grid": [50],
            "max_depth_grid": [3],
            "learning_rate_grid": [0.1],
            "subsample_grid": [0.8],
            "colsample_bytree_grid": [0.8],
        },
        "ensemble": {
            "method": "stacking",
            "base_models": ["LR_EN", "RF"],
            "meta_model": {
                "type": "logistic_regression",
                "penalty": "l2",
                "C": 1.0,
            },
        },
    }

    config_path = tmp_path / "minimal_training_config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    return config_path


@pytest.fixture
def minimal_splits_config(tmp_path):
    """Create minimal splits config."""
    config = {
        "mode": "development",
        "scenarios": ["IncidentOnly"],
        "n_splits": 2,
        "val_size": 0.25,
        "test_size": 0.25,
        "seed_start": 42,
        "train_control_per_case": 5.0,
        "prevalent_train_only": False,
    }

    config_path = tmp_path / "minimal_splits_config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    return config_path


@pytest.fixture
def hpc_config(tmp_path):
    """Create HPC pipeline config for dry-run testing."""
    config = {
        "environment": "hpc",
        "paths": {
            "infile": "../data/test.parquet",
            "splits_dir": "../splits",
            "results_dir": "../results",
        },
        "hpc": {
            "project": "TEST_ALLOCATION",
            "queue": "short",
            "cores": 4,
            "memory": "8G",
            "walltime": "02:00",
        },
        "execution": {
            "models": ["LR_EN", "RF"],
            "n_boot": 100,
            "overwrite_splits": False,
        },
    }

    config_path = tmp_path / "hpc_config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config, f)

    return config_path


# ==================== Test Classes ====================


class TestE2EFullPipeline:
    """Test full pipeline: splits -> train -> aggregate."""

    def test_splits_generation_basic(self, minimal_proteomics_data, tmp_path):
        """
        Test: Generate splits and verify output structure.

        Validates split files, metadata, and reproducibility.
        """
        splits_dir = tmp_path / "splits"
        splits_dir.mkdir()

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--mode",
                "development",
                "--scenarios",
                "IncidentOnly",
                "--n-splits",
                "2",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )

        assert result.exit_code == 0, f"save-splits failed: {result.output}"

        # Verify files exist for both splits
        for seed in [42, 43]:
            assert (splits_dir / f"train_idx_IncidentOnly_seed{seed}.csv").exists()
            assert (splits_dir / f"val_idx_IncidentOnly_seed{seed}.csv").exists()
            assert (splits_dir / f"test_idx_IncidentOnly_seed{seed}.csv").exists()
            assert (splits_dir / f"split_meta_IncidentOnly_seed{seed}.json").exists()

        # Verify metadata
        with open(splits_dir / "split_meta_IncidentOnly_seed42.json") as f:
            meta = json.load(f)

        assert meta["scenario"] == "IncidentOnly"
        assert meta["seed"] == 42
        assert meta["split_type"] == "development"
        assert meta["n_train"] > 0
        assert meta["n_val"] > 0
        assert meta["n_test"] > 0

    def test_reproducibility_same_seed(self, minimal_proteomics_data, tmp_path):
        """
        Test: Same seed produces identical splits.

        Critical for reproducibility verification.
        """
        splits_dir1 = tmp_path / "splits1"
        splits_dir2 = tmp_path / "splits2"
        splits_dir1.mkdir()
        splits_dir2.mkdir()

        runner = CliRunner()

        # Run 1
        result1 = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir1),
                "--mode",
                "development",
                "--scenarios",
                "IncidentOnly",
                "--n-splits",
                "1",
                "--seed-start",
                "123",
            ],
        )
        assert result1.exit_code == 0

        # Run 2
        result2 = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir2),
                "--mode",
                "development",
                "--scenarios",
                "IncidentOnly",
                "--n-splits",
                "1",
                "--seed-start",
                "123",
            ],
        )
        assert result2.exit_code == 0

        # Compare splits
        train1 = pd.read_csv(splits_dir1 / "train_idx_IncidentOnly_seed123.csv")["idx"].values
        train2 = pd.read_csv(splits_dir2 / "train_idx_IncidentOnly_seed123.csv")["idx"].values

        np.testing.assert_array_equal(train1, train2)

    @pytest.mark.slow
    def test_full_pipeline_single_model(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Full pipeline with one model (splits -> train -> results).

        This is the core E2E test. Marked slow (~30-60s).
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Step 1: Generate splits
        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--mode",
                "development",
                "--scenarios",
                "IncidentOnly",
                "--n-splits",
                "1",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )
        assert result_splits.exit_code == 0, f"Splits failed: {result_splits.output}"

        # Step 2: Train model
        result_train = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
            ],
            catch_exceptions=False,
        )

        if result_train.exit_code != 0:
            print("TRAIN OUTPUT:", result_train.output)
            if result_train.exception:
                import traceback

                traceback.print_exception(
                    type(result_train.exception),
                    result_train.exception,
                    result_train.exception.__traceback__,
                )

        assert result_train.exit_code == 0, f"Train failed: {result_train.output}"

        # Step 3: Verify outputs
        # Find the run directory (timestamped run_YYYYMMDD_HHMMSS)
        run_dirs = [d for d in results_dir.iterdir() if d.is_dir() and d.name.startswith("run_")]
        assert len(run_dirs) == 1, f"Expected 1 run directory, found {len(run_dirs)}: {run_dirs}"
        model_dir = run_dirs[0] / "splits" / "split_seed42"
        assert model_dir.exists(), f"Model directory not found: {model_dir}"

        # Check required output files
        required_files = [
            "core/val_metrics.csv",
            "core/test_metrics.csv",
            "preds/train_oof__LR_EN.csv",
            "preds/test_preds__LR_EN.csv",
        ]

        for file_path in required_files:
            full_path = model_dir / file_path
            assert full_path.exists(), f"Missing output: {full_path}"

        # Validate metrics structure
        test_metrics = pd.read_csv(model_dir / "core/test_metrics.csv")

        # Check for expected metric columns (try both uppercase and lowercase)
        has_auroc = any(col.lower() == "auroc" for col in test_metrics.columns)
        has_metric_col = "metric" in test_metrics.columns

        assert (
            has_auroc or has_metric_col
        ), f"No AUROC column found. Columns: {test_metrics.columns.tolist()}"

        # If it's a long-format metrics file, check for auroc row
        if has_metric_col:
            assert any(val.lower() == "auroc" for val in test_metrics["metric"].values)
            auroc_val = test_metrics[test_metrics["metric"].str.lower() == "auroc"]["value"].iloc[0]
        else:
            # Find the AUROC column (case-insensitive)
            auroc_col = [col for col in test_metrics.columns if col.lower() == "auroc"][0]
            auroc_val = test_metrics[auroc_col].iloc[0]

        assert 0.0 <= auroc_val <= 1.0, f"AUROC out of bounds: {auroc_val}"

    def test_output_file_structure(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Verify complete output file structure after training.

        Ensures all expected outputs are generated.
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Generate splits
        runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )

        # Train
        result = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
            ],
            catch_exceptions=False,
        )

        if result.exit_code != 0:
            pytest.skip(f"Training failed, skipping structure check: {result.output[:200]}")

        # Find the actual output directory (may be under run_YYYYMMDD_HHMMSS/splits/)
        model_dirs = list(results_dir.rglob("splits/split_seed42"))

        if not model_dirs:
            all_files = list(results_dir.rglob("*"))
            pytest.skip(
                f"No split_seed42 directory found. Files: {[str(f.relative_to(results_dir)) for f in all_files[:10]]}"
            )

        model_dir = model_dirs[0]

        # Verify key outputs exist (flexible check for different structures)
        has_predictions = any(model_dir.rglob("*.csv"))
        has_config = any(model_dir.rglob("*config*.yaml"))
        has_some_output = len(list(model_dir.rglob("*"))) > 5

        assert has_predictions, "No CSV files (predictions) found"
        assert has_config, "No config YAML found"
        assert has_some_output, "Output directory is mostly empty"


class TestE2EEnsembleWorkflow:
    """Test ensemble workflow: base models -> stacking -> aggregate."""

    @pytest.mark.slow
    def test_ensemble_training_workflow(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Train base models then ensemble meta-learner.

        Critical ensemble integration test. Marked slow (~2-3 min).
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Step 1: Generate splits
        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )
        assert result_splits.exit_code == 0

        # Step 2: Train base models (LR_EN and RF)
        base_models = ["LR_EN", "RF"]

        for model in base_models:
            result_train = runner.invoke(
                cli,
                [
                    "train",
                    "--infile",
                    str(minimal_proteomics_data),
                    "--split-dir",
                    str(splits_dir),
                    "--outdir",
                    str(results_dir),
                    "--config",
                    str(minimal_training_config),
                    "--model",
                    model,
                    "--split-seed",
                    "42",
                ],
                catch_exceptions=False,
            )

            if result_train.exit_code != 0:
                pytest.skip(f"Base model {model} training failed: {result_train.output[:200]}")

        # Step 3: Train ensemble
        result_ensemble = runner.invoke(
            cli,
            [
                "train-ensemble",
                "--results-dir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--base-models",
                ",".join(base_models),
                "--split-seed",
                "42",
            ],
            catch_exceptions=False,
        )

        if result_ensemble.exit_code != 0:
            print("ENSEMBLE OUTPUT:", result_ensemble.output)
            if result_ensemble.exception:
                import traceback

                traceback.print_exception(
                    type(result_ensemble.exception),
                    result_ensemble.exception,
                    result_ensemble.exception.__traceback__,
                )

        assert result_ensemble.exit_code == 0, f"Ensemble failed: {result_ensemble.output}"

        # Verify ensemble outputs
        # Ensemble creates output in ENSEMBLE/split_{seed} directory (not timestamped run dirs)
        ensemble_dir = results_dir / "ENSEMBLE" / "split_42"
        assert ensemble_dir.exists(), f"Ensemble directory not found: {ensemble_dir}"

        # Check ensemble-specific files (using actual file structure)
        assert (ensemble_dir / "core/metrics.json").exists(), "Missing metrics.json"
        assert (
            ensemble_dir / "preds/test_preds__ENSEMBLE.csv"
        ).exists(), "Missing test predictions"

    def test_ensemble_requires_base_models(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Ensemble training fails gracefully without base models.

        Error handling test.
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Generate splits
        runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
            ],
        )

        # Try to train ensemble without base models
        result = runner.invoke(
            cli,
            [
                "train-ensemble",
                "--results-dir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--base-models",
                "LR_EN,RF",
                "--split-seed",
                "42",
            ],
        )

        # Should fail with informative error
        assert result.exit_code != 0
        assert "base model" in result.output.lower() or "not found" in result.output.lower()


class TestE2EHPCWorkflow:
    """Test HPC workflow validation (dry-run mode)."""

    def test_hpc_config_validation(self, hpc_config):
        """
        Test: HPC config loads and validates correctly.

        Validates config structure without execution.
        """
        with open(hpc_config) as f:
            config = yaml.safe_load(f)

        # Check required HPC fields
        assert "hpc" in config
        assert "project" in config["hpc"]
        assert "queue" in config["hpc"]
        assert "cores" in config["hpc"]
        assert "memory" in config["hpc"]

        # Validate types
        assert isinstance(config["hpc"]["cores"], int)
        assert config["hpc"]["cores"] > 0

    def test_hpc_dry_run_mode(self, minimal_proteomics_data, minimal_training_config, tmp_path):
        """
        Test: Dry-run mode shows what would be executed without running.

        This is tested via checking config loading logic.
        """
        # This test validates that configs can be loaded for HPC submission
        # Actual dry-run would require run_hpc.sh which we can't easily test

        with open(minimal_training_config) as f:
            config = yaml.safe_load(f)

        # Verify config has all required sections
        assert "cv" in config
        assert "features" in config
        assert "calibration" in config

        # Verify models are specified
        assert "lr" in config or "LR" in str(config)


class TestE2ETemporalValidation:
    """Test temporal validation workflow."""

    def test_temporal_splits_generation(self, temporal_proteomics_data, tmp_path):
        """
        Test: Generate temporal splits with chronological ordering.

        Validates temporal split logic.
        """
        splits_dir = tmp_path / "splits_temporal"
        splits_dir.mkdir()

        # Create temporal config
        config = {
            "mode": "development",
            "scenarios": ["IncidentOnly"],
            "n_splits": 1,
            "temporal_split": True,
            "temporal_column": "sample_date",
            "train_frac": 0.7,
            "val_frac": 0.15,
            "test_frac": 0.15,
        }

        config_path = tmp_path / "temporal_config.yaml"
        with open(config_path, "w") as f:
            yaml.dump(config, f)

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(temporal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--config",
                str(config_path),
            ],
        )

        # Note: Current implementation may not fully support temporal splits in CLI
        # This test documents expected behavior
        if result.exit_code != 0:
            pytest.skip(f"Temporal splits not fully implemented in CLI: {result.output[:200]}")

        # If implemented, verify chronological ordering
        df = pd.read_parquet(temporal_proteomics_data)
        train_idx = pd.read_csv(splits_dir / "train_idx_IncidentOnly_seed0.csv")["idx"].values
        test_idx = pd.read_csv(splits_dir / "test_idx_IncidentOnly_seed0.csv")["idx"].values

        # Train should have earliest dates
        train_dates = df.loc[train_idx, "sample_date"]
        test_dates = df.loc[test_idx, "sample_date"]

        assert train_dates.max() < test_dates.min(), "Temporal ordering violated"


class TestE2EErrorHandling:
    """Test error handling for common failure modes."""

    def test_missing_input_file(self, tmp_path):
        """Test: Graceful error for missing input file."""
        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(tmp_path / "nonexistent.parquet"),
                "--outdir",
                str(tmp_path / "splits"),
            ],
        )

        assert result.exit_code != 0
        assert "not found" in result.output.lower() or "does not exist" in result.output.lower()

    def test_invalid_model_name(self, minimal_proteomics_data, minimal_training_config, tmp_path):
        """Test: Graceful error for invalid model name."""
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Generate splits
        runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
            ],
        )

        # Try to train with invalid model
        result = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "INVALID_MODEL_XYZ",
                "--split-seed",
                "42",
            ],
        )

        assert result.exit_code != 0
        assert "model" in result.output.lower() or "invalid" in result.output.lower()

    def test_missing_splits_dir(self, minimal_proteomics_data, minimal_training_config, tmp_path):
        """Test: Graceful error when splits directory missing."""
        results_dir = tmp_path / "results"
        results_dir.mkdir()

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(tmp_path / "nonexistent_splits"),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
            ],
        )

        assert result.exit_code != 0

    def test_corrupted_config(self, minimal_proteomics_data, tmp_path):
        """Test: Graceful error for corrupted config file."""
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        # Create corrupted config
        bad_config = tmp_path / "bad_config.yaml"
        with open(bad_config, "w") as f:
            f.write("{ invalid yaml content: [ unclosed")

        runner = CliRunner()

        # Generate splits
        runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
            ],
        )

        # Try to train with bad config
        result = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(bad_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
            ],
        )

        assert result.exit_code != 0


class TestE2EPanelOptimization:
    """Test panel optimization workflow: train -> optimize-panel -> validate."""

    @pytest.mark.slow
    def test_panel_optimization_workflow(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Panel optimization via RFE after training and aggregation.

        Critical workflow for deployment sizing. Marked slow (~1-2 min).
        Tests the aggregated panel optimization path (ADR-013 compliant).
        """
        import os

        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        # Set environment variable for CLI to find results
        env = os.environ.copy()
        env["CED_RESULTS_DIR"] = str(results_dir)

        runner = CliRunner(env=env)

        # Step 1: Generate splits
        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )
        assert result_splits.exit_code == 0

        # Step 2: Train model with run-id
        run_id = "test_panel_opt"
        result_train = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir / "LR_EN"),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
                "--run-id",
                run_id,
            ],
            catch_exceptions=False,
        )

        if result_train.exit_code != 0:
            pytest.skip(f"Training failed: {result_train.output[:200]}")

        # Step 3: Aggregate results (required for panel optimization)
        result_agg = runner.invoke(
            cli,
            ["aggregate-splits", "--run-id", run_id, "--model", "LR_EN"],
            catch_exceptions=False,
        )

        if result_agg.exit_code != 0:
            pytest.skip(f"Aggregation failed: {result_agg.output[:200]}")

        # Step 4: Run panel optimization on aggregated results
        result_optimize = runner.invoke(
            cli,
            [
                "optimize-panel",
                "--run-id",
                run_id,
                "--model",
                "LR_EN",
                "--min-size",
                "3",
            ],
            catch_exceptions=False,
        )

        if result_optimize.exit_code != 0:
            print("OPTIMIZE OUTPUT:", result_optimize.output)
            if result_optimize.exception:
                import traceback

                traceback.print_exception(
                    type(result_optimize.exception),
                    result_optimize.exception,
                    result_optimize.exception.__traceback__,
                )

        assert (
            result_optimize.exit_code == 0
        ), f"Panel optimization failed: {result_optimize.output}"

        # Verify RFE outputs in aggregated directory
        panel_dir = results_dir / "LR_EN" / f"run_{run_id}" / "aggregated" / "optimize_panel"
        assert panel_dir.exists(), "Panel optimization directory not found"
        assert (panel_dir / "panel_curve_aggregated.csv").exists(), "Missing panel curve"
        assert (panel_dir / "feature_ranking_aggregated.csv").exists(), "Missing feature ranking"
        assert (
            panel_dir / "recommended_panels_aggregated.json"
        ).exists(), "Missing recommendations"

        # Validate panel curve structure
        panel_curve = pd.read_csv(panel_dir / "panel_curve_aggregated.csv")
        assert "size" in panel_curve.columns
        assert "auroc_val" in panel_curve.columns
        assert len(panel_curve) > 0

        # Check that AUROC is in valid range
        assert all(0.0 <= auc <= 1.0 for auc in panel_curve["auroc_val"])

    def test_panel_optimization_requires_aggregated_results(
        self, minimal_proteomics_data, tmp_path
    ):
        """
        Test: Panel optimization fails gracefully without aggregated results.

        Error handling test for missing aggregation step.
        """
        import os

        results_dir = tmp_path / "results"
        results_dir.mkdir()

        # Set environment variable
        env = os.environ.copy()
        env["CED_RESULTS_DIR"] = str(results_dir)

        runner = CliRunner(env=env)

        # Try to optimize panel with non-existent run-id
        result = runner.invoke(
            cli,
            [
                "optimize-panel",
                "--run-id",
                "nonexistent_run",
            ],
        )

        # Should fail with informative error
        assert result.exit_code != 0
        assert "not found" in result.output.lower() or "no models" in result.output.lower()


class TestE2EFixedPanelValidation:
    """Test fixed panel validation workflow: train with --fixed-panel."""

    @pytest.mark.slow
    def test_fixed_panel_training_workflow(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Train model with fixed panel (bypasses feature selection).

        Critical for panel validation. Marked slow (~30-60s).
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Step 1: Generate splits
        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )
        assert result_splits.exit_code == 0

        # Step 2: Create a fixed panel file (5 proteins)
        panel_file = tmp_path / "fixed_panel.csv"
        panel_proteins = [f"PROT_{i:03d}_resid" for i in range(5)]
        pd.DataFrame({"protein": panel_proteins}).to_csv(panel_file, index=False)

        # Step 3: Train with fixed panel
        result_train = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
                "--fixed-panel",
                str(panel_file),
            ],
            catch_exceptions=False,
        )

        if result_train.exit_code != 0:
            print("TRAIN OUTPUT:", result_train.output)
            if result_train.exception:
                import traceback

                traceback.print_exception(
                    type(result_train.exception),
                    result_train.exception,
                    result_train.exception.__traceback__,
                )

        assert result_train.exit_code == 0, f"Fixed panel training failed: {result_train.output}"

        # Verify outputs exist
        run_dirs = [d for d in results_dir.iterdir() if d.is_dir() and d.name.startswith("run_")]
        assert len(run_dirs) == 1, f"Expected 1 run directory, found {len(run_dirs)}"
        model_dir = run_dirs[0] / "splits" / "split_seed42"
        assert model_dir.exists()

        # Check that model was trained
        has_predictions = any(model_dir.rglob("*.csv"))
        has_config = any(model_dir.rglob("*config*.yaml"))

        assert has_predictions, "No predictions found"
        assert has_config, "No config file found"

    def test_fixed_panel_invalid_file(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Fixed panel training fails gracefully with invalid panel file.

        Error handling test.
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Generate splits
        runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "1",
            ],
        )

        # Try to train with nonexistent panel file
        result = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
                "--fixed-panel",
                str(tmp_path / "nonexistent_panel.csv"),
            ],
        )

        # Should fail with informative error
        assert result.exit_code != 0
        assert "not found" in result.output.lower() or "does not exist" in result.output.lower()


class TestE2EAggregationWorkflow:
    """Test aggregation workflow: multiple splits -> aggregate results."""

    @pytest.mark.slow
    def test_aggregation_across_splits(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Aggregate results across multiple split seeds.

        Critical for multi-split analysis. Marked slow (~2-3 min).
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        splits_dir.mkdir()
        results_dir.mkdir()

        runner = CliRunner()

        # Step 1: Generate 2 splits
        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--n-splits",
                "2",
                "--val-size",
                "0.25",
                "--test-size",
                "0.25",
                "--seed-start",
                "42",
            ],
        )
        assert result_splits.exit_code == 0

        # Step 2: Train on both splits
        for seed in [42, 43]:
            result_train = runner.invoke(
                cli,
                [
                    "train",
                    "--infile",
                    str(minimal_proteomics_data),
                    "--split-dir",
                    str(splits_dir),
                    "--outdir",
                    str(results_dir),
                    "--config",
                    str(minimal_training_config),
                    "--model",
                    "LR_EN",
                    "--split-seed",
                    str(seed),
                ],
                catch_exceptions=False,
            )

            if result_train.exit_code != 0:
                pytest.skip(f"Training on seed {seed} failed: {result_train.output[:200]}")

        # Step 3: Create aggregation config
        agg_config = {
            "model": "LR_EN",
            "split_seeds": [42, 43],
            "n_bootstrap": 100,
        }
        agg_config_path = tmp_path / "agg_config.yaml"
        with open(agg_config_path, "w") as f:
            yaml.dump(agg_config, f)

        # Step 4: Run aggregation
        result_agg = runner.invoke(
            cli,
            [
                "aggregate-splits",
                "--results-dir",
                str(results_dir),
                "--config",
                str(agg_config_path),
            ],
            catch_exceptions=False,
        )

        if result_agg.exit_code != 0:
            print("AGGREGATE OUTPUT:", result_agg.output)
            if result_agg.exception:
                import traceback

                traceback.print_exception(
                    type(result_agg.exception),
                    result_agg.exception,
                    result_agg.exception.__traceback__,
                )
            pytest.skip(
                f"Aggregation not yet fully implemented or failed: {result_agg.output[:200]}"
            )

        assert result_agg.exit_code == 0, f"Aggregation failed: {result_agg.output}"

        # Verify aggregated outputs
        # Look for aggregated directory
        agg_dirs = list(results_dir.rglob("*aggregated*"))
        if agg_dirs:
            agg_dir = agg_dirs[0]
            # Check for expected aggregation outputs
            has_metrics = any(agg_dir.rglob("*metrics*.csv")) or any(
                agg_dir.rglob("*metrics*.json")
            )
            assert has_metrics, "No aggregated metrics found"

    def test_aggregation_requires_multiple_splits(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Aggregation fails gracefully without sufficient splits.

        Error handling test.
        """
        results_dir = tmp_path / "results"
        results_dir.mkdir()

        runner = CliRunner()

        # Create minimal aggregation config
        agg_config = {
            "model": "LR_EN",
            "split_seeds": [42, 43],
            "n_bootstrap": 100,
        }
        agg_config_path = tmp_path / "agg_config.yaml"
        with open(agg_config_path, "w") as f:
            yaml.dump(agg_config, f)

        # Try to aggregate without training any splits
        result = runner.invoke(
            cli,
            [
                "aggregate-splits",
                "--results-dir",
                str(results_dir),
                "--config",
                str(agg_config_path),
            ],
        )

        # Should fail (either immediately or with clear error)
        assert result.exit_code != 0


class TestE2EConfigValidation:
    """Test config validation workflow."""

    def test_config_validate_valid_training_config(self, minimal_training_config):
        """
        Test: Config validation passes for valid training config.

        Validates config validation command.
        """
        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "validate",
                str(minimal_training_config),
                "--command",
                "train",
            ],
        )

        assert result.exit_code == 0, f"Validation should pass: {result.output}"

    def test_config_validate_invalid_config(self, tmp_path):
        """
        Test: Config validation fails for invalid config.

        Error handling test for malformed configs.
        """
        bad_config = tmp_path / "bad_config.yaml"
        with open(bad_config, "w") as f:
            yaml.dump({"invalid": "structure", "missing": "required_fields"}, f)

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "validate",
                str(bad_config),
                "--command",
                "train",
            ],
        )

        # Should fail or warn
        # Note: May exit with 0 if warnings only, check output
        assert (
            result.exit_code != 0 or "warning" in result.output.lower()
        ), "Should warn or fail for invalid config"

    def test_config_validate_strict_mode(self, minimal_training_config):
        """
        Test: Strict mode treats warnings as errors.

        Validates strict validation behavior.
        """
        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "validate",
                str(minimal_training_config),
                "--command",
                "train",
                "--strict",
            ],
        )

        # In strict mode, any warnings should cause failure
        # (or pass if config is perfect)
        assert result.exit_code in [
            0,
            1,
        ], "Strict validation should exit with 0 (pass) or 1 (fail)"

    def test_config_diff_identical_configs(self, minimal_training_config, tmp_path):
        """
        Test: Config diff shows no differences for identical configs.

        Validates config comparison command.
        """
        # Create a copy of the config
        config_copy = tmp_path / "config_copy.yaml"
        with open(minimal_training_config) as f:
            config_data = yaml.safe_load(f)
        with open(config_copy, "w") as f:
            yaml.dump(config_data, f)

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "diff",
                str(minimal_training_config),
                str(config_copy),
            ],
        )

        assert result.exit_code == 0
        assert "identical" in result.output.lower() or "no diff" in result.output.lower()

    def test_config_diff_different_configs(self, minimal_training_config, minimal_splits_config):
        """
        Test: Config diff shows differences for different configs.

        Validates diff detection.
        """
        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "diff",
                str(minimal_training_config),
                str(minimal_splits_config),
            ],
        )

        assert result.exit_code == 0
        # Should show differences
        assert len(result.output) > 100, "Diff output should show differences"

    def test_config_diff_output_file(
        self, minimal_training_config, minimal_splits_config, tmp_path
    ):
        """
        Test: Config diff can write to output file.

        Validates output file option.
        """
        output_file = tmp_path / "diff_report.txt"

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "config",
                "diff",
                str(minimal_training_config),
                str(minimal_splits_config),
                "--output",
                str(output_file),
            ],
        )

        assert result.exit_code == 0
        assert output_file.exists(), "Diff output file should be created"
        assert output_file.stat().st_size > 0, "Diff output should have content"


class TestE2EHoldoutEvaluation:
    """Test holdout evaluation workflow."""

    @pytest.mark.slow
    def test_eval_holdout_workflow(
        self, minimal_proteomics_data, minimal_training_config, tmp_path
    ):
        """
        Test: Holdout evaluation workflow (train â†’ eval-holdout).

        Critical for final model validation. Marked slow (~2 min).
        """
        splits_dir = tmp_path / "splits"
        results_dir = tmp_path / "results"
        holdout_dir = tmp_path / "holdout_results"
        splits_dir.mkdir()
        results_dir.mkdir()
        holdout_dir.mkdir()

        runner = CliRunner()

        # Step 1: Generate splits with holdout
        # Note: Using development mode (no holdout) first to test eval-holdout CLI
        # Holdout stratification requires many more samples to avoid single-group strata
        holdout_splits_config = {
            "mode": "development",
            "scenarios": ["IncidentOnly"],
            "n_splits": 1,
            "val_size": 0.25,
            "test_size": 0.25,
            "seed_start": 42,
        }
        holdout_config_path = tmp_path / "holdout_splits_config.yaml"
        with open(holdout_config_path, "w") as f:
            yaml.dump(holdout_splits_config, f)

        result_splits = runner.invoke(
            cli,
            [
                "save-splits",
                "--infile",
                str(minimal_proteomics_data),
                "--outdir",
                str(splits_dir),
                "--config",
                str(holdout_config_path),
            ],
        )
        assert result_splits.exit_code == 0, f"Splits failed: {result_splits.output}"

        # Create a fake holdout indices file from test set indices
        # (In real workflow, holdout would come from save-splits with mode=holdout)
        test_idx_file = splits_dir / "test_idx_IncidentOnly_seed42.csv"
        assert test_idx_file.exists(), "Test indices should exist"
        test_idx_df = pd.read_csv(test_idx_file)
        holdout_idx_file = splits_dir / "holdout_idx_IncidentOnly_seed42.csv"
        test_idx_df.to_csv(holdout_idx_file, index=False)

        # Step 2: Train model
        result_train = runner.invoke(
            cli,
            [
                "train",
                "--infile",
                str(minimal_proteomics_data),
                "--split-dir",
                str(splits_dir),
                "--outdir",
                str(results_dir),
                "--config",
                str(minimal_training_config),
                "--model",
                "LR_EN",
                "--split-seed",
                "42",
            ],
            catch_exceptions=False,
        )

        if result_train.exit_code != 0:
            pytest.skip(f"Training failed: {result_train.output[:200]}")

        # Find trained model
        model_files = list(results_dir.rglob("*final_model*.joblib"))
        if not model_files:
            model_files = list(results_dir.rglob("*.joblib"))
        if not model_files:
            pytest.skip("No trained model found")

        model_path = model_files[0]

        # Step 3: Evaluate on holdout
        result_holdout = runner.invoke(
            cli,
            [
                "eval-holdout",
                "--infile",
                str(minimal_proteomics_data),
                "--model-artifact",
                str(model_path),
                "--holdout-idx",
                str(holdout_idx_file),
                "--outdir",
                str(holdout_dir),
            ],
            catch_exceptions=False,
        )

        if result_holdout.exit_code != 0:
            print("HOLDOUT OUTPUT:", result_holdout.output)
            if result_holdout.exception:
                import traceback

                traceback.print_exception(
                    type(result_holdout.exception),
                    result_holdout.exception,
                    result_holdout.exception.__traceback__,
                )

        assert result_holdout.exit_code == 0, f"Holdout eval failed: {result_holdout.output}"

        # Verify holdout outputs
        assert any(holdout_dir.rglob("*")), "Holdout output directory should contain files"
        # Check for metrics (could be metrics.json or metrics.csv)
        has_metrics = any(holdout_dir.rglob("*metrics*")) or any(holdout_dir.rglob("*.json"))
        assert has_metrics, "Holdout metrics file should exist"

    def test_eval_holdout_missing_model(self, minimal_proteomics_data, tmp_path):
        """
        Test: Holdout evaluation fails gracefully with missing model.

        Error handling test.
        """
        splits_dir = tmp_path / "splits"
        holdout_dir = tmp_path / "holdout_results"
        splits_dir.mkdir()
        holdout_dir.mkdir()

        # Create fake holdout indices
        holdout_idx = pd.DataFrame({"idx": [0, 1, 2, 3, 4]})
        holdout_idx_file = splits_dir / "holdout_idx.csv"
        holdout_idx.to_csv(holdout_idx_file, index=False)

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "eval-holdout",
                "--infile",
                str(minimal_proteomics_data),
                "--model-artifact",
                str(tmp_path / "nonexistent_model.joblib"),
                "--holdout-idx",
                str(holdout_idx_file),
                "--outdir",
                str(holdout_dir),
            ],
        )

        assert result.exit_code != 0
        assert "not found" in result.output.lower() or "does not exist" in result.output.lower()

    def test_eval_holdout_compute_dca_flag(self, minimal_proteomics_data, tmp_path):
        """
        Test: Holdout evaluation with --compute-dca flag.

        Validates optional DCA computation.
        """
        # This test would require a trained model
        # Skipping actual execution, just testing flag parsing
        runner = CliRunner()

        # Just verify the flag is recognized (will fail on missing files)
        result = runner.invoke(
            cli,
            [
                "eval-holdout",
                "--infile",
                str(minimal_proteomics_data),
                "--model-artifact",
                str(tmp_path / "fake_model.joblib"),
                "--holdout-idx",
                str(tmp_path / "fake_holdout.csv"),
                "--outdir",
                str(tmp_path / "out"),
                "--compute-dca",
            ],
        )

        # Should fail on missing files, but flag should be recognized
        assert "--compute-dca" not in result.output or result.exit_code != 0


class TestE2EDataConversion:
    """Test data format conversion utilities."""

    def test_convert_to_parquet_basic(self, minimal_proteomics_data, tmp_path):
        """
        Test: CSV to Parquet conversion.

        Validates data format conversion command.
        """
        # Create CSV version from parquet fixture
        csv_path = tmp_path / "input.csv"
        df = pd.read_parquet(minimal_proteomics_data)
        df.to_csv(csv_path, index=False)

        output_path = tmp_path / "output.parquet"

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "convert-to-parquet",
                str(csv_path),
                "--output",
                str(output_path),
            ],
        )

        assert result.exit_code == 0, f"Conversion failed: {result.output}"
        assert output_path.exists(), "Output parquet file should exist"

        # Verify content
        df_converted = pd.read_parquet(output_path)
        assert len(df_converted) == len(df), "Row count should match"
        assert len(df_converted.columns) == len(df.columns), "Column count should match"

    def test_convert_to_parquet_default_output(self, tmp_path):
        """
        Test: Conversion with default output path.

        Validates automatic output path generation.
        """
        # Create minimal CSV
        csv_path = tmp_path / "test_data.csv"
        df = pd.DataFrame(
            {
                ID_COL: ["S001", "S002"],
                TARGET_COL: [CONTROL_LABEL, INCIDENT_LABEL],
                "protein_001_resid": [1.0, 2.0],
            }
        )
        df.to_csv(csv_path, index=False)

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "convert-to-parquet",
                str(csv_path),
            ],
        )

        assert result.exit_code == 0

        # Check default output path (same as input with .parquet extension)
        expected_output = tmp_path / "test_data.parquet"
        assert expected_output.exists(), "Default output should be created"

    def test_convert_to_parquet_compression_options(self, tmp_path):
        """
        Test: Parquet conversion with different compression algorithms.

        Validates compression option.
        """
        csv_path = tmp_path / "input.csv"
        rng = np.random.default_rng(42)
        # Need at least one case sample for validation
        labels = [CONTROL_LABEL] * 45 + [INCIDENT_LABEL] * 5
        df = pd.DataFrame(
            {
                ID_COL: [f"S{i:03d}" for i in range(50)],
                TARGET_COL: labels,
                "age": rng.integers(25, 75, 50),
                "BMI": rng.uniform(18, 35, 50),
                "sex": rng.choice(["M", "F"], 50),
                "Genetic ethnic grouping": rng.choice(["White", "Asian"], 50),
                "protein_001_resid": np.random.randn(50),
                "protein_002_resid": np.random.randn(50),
            }
        )
        df.to_csv(csv_path, index=False)

        # Test different compression algorithms
        for compression in ["snappy", "gzip", "zstd"]:
            output_path = tmp_path / f"output_{compression}.parquet"

            runner = CliRunner()
            result = runner.invoke(
                cli,
                [
                    "convert-to-parquet",
                    str(csv_path),
                    "--output",
                    str(output_path),
                    "--compression",
                    compression,
                ],
            )

            if result.exit_code != 0 and "not available" in result.output:
                pytest.skip(f"Compression {compression} not available in environment")

            assert result.exit_code == 0, f"Conversion with {compression} failed"
            assert output_path.exists(), f"Output with {compression} should exist"

    def test_convert_to_parquet_invalid_csv(self, tmp_path):
        """
        Test: Conversion fails gracefully with invalid CSV.

        Error handling test.
        """
        bad_csv = tmp_path / "bad.csv"
        with open(bad_csv, "w") as f:
            f.write("invalid,csv,structure\nno,proper,headers\n")

        runner = CliRunner()
        result = runner.invoke(
            cli,
            [
                "convert-to-parquet",
                str(bad_csv),
            ],
        )

        # May fail or succeed depending on CSV content
        # Just ensure it doesn't crash
        assert result.exit_code in [0, 1]


# ==================== How to Run ====================
# Fast tests only:
#   pytest tests/test_e2e_runner.py -v -m "not slow"
#
# All tests including slow:
#   pytest tests/test_e2e_runner.py -v
#
# Specific test class:
#   pytest tests/test_e2e_runner.py::TestE2EFullPipeline -v
#
# Single test:
#   pytest tests/test_e2e_runner.py::TestE2EFullPipeline::test_splits_generation_basic -v
