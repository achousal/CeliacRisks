# Training Configuration - L2 Multi-Objective (1-Hour Runtime)
# Multi-objective Optuna optimization (AUROC + Brier) with reduced trial count.
# Optimized for 1-hour parallel execution on 4 cores.
# Goal: Explore discrimination-calibration tradeoffs efficiently.

scenario: IncidentPlusPrevalent

# Cross-validation: reduced for speed (3 folds, 3 repeats)
cv:
  folds: 3                    # down from 5 (faster CV, adequate for ranking)
  repeats: 3                  # down from 5 (3 repeats sufficient for multi-objective)
  inner_folds: 3              # down from 5 (inner tuning loop)
  scoring: roc_auc
  n_jobs: -1
  random_state: 42
  grid_randomize: true
  verbose: 1

# Ensemble (trained separately via train-ensemble CLI)
ensemble:
  method: stacking
  base_models: [LR_EN, RF, XGBoost, LinSVM_cal]
  meta_model:
    type: logistic_regression
    penalty: l2
    C: 1.0
    max_iter: 1000
    solver: lbfgs
  use_probabilities: true
  passthrough: false
  cv_for_meta: 5

# Optuna: multi-objective AUROC + Brier with 20 trials for 1-hour constraint
optuna:
  enabled: true
  multi_objective: true                              # Enable multi-objective optimization
  objectives: ["roc_auc", "neg_brier_score"]         # Discrimination + calibration
  pareto_selection: "knee"                           # Knee point (balanced tradeoff)
  n_trials: 20                                       # down from 30 (1-hour constraint)
  timeout: null
  sampler: tpe
  sampler_seed: 42
  pruner: hyperband
  pruner_n_startup_trials: 5                         # reduced warmup (20 trials total)
  pruner_percentile: 25.0
  n_jobs: -1
  save_study: true
  save_trials_csv: true

# Feature selection: reduced k_grid for speed
features:
  feature_select: hybrid
  kbest_scope: protein
  kbest_max: 600                                     # down from 800
  k_grid: [50, 100, 200, 400, 600]                   # 5 points (down from 9)
  l1_c_min: 0.001
  l1_c_max: 1.0
  l1_c_points: 4
  l1_stability_thresh: 0.70
  hybrid_kbest_first: true
  hybrid_k_for_stability: 200
  screen_method: mannwhitney
  screen_top_n: 1000
  stability_thresh: 0.70
  stable_corr_thresh: 0.85

# Logistic Regression (ElasticNet): continuous Optuna ranges
lr:
  C_min: 0.0001
  C_max: 10.0
  C_points: 20
  l1_ratio: [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]           # reduced from 10 values to 6
  class_weight_options: "balanced"
  optuna_C: [1.0e-5, 100.0]
  optuna_l1_ratio: [0.0, 1.0]
  solver: saga
  max_iter: 10000
  n_iter: 4

# Linear SVM: continuous Optuna ranges
svm:
  C_min: 0.0001
  C_max: 10.0
  C_points: 15
  class_weight_options: "balanced"
  optuna_C: [1.0e-4, 100.0]
  max_iter: 10000
  n_iter: 6

# Random Forest: pruned grid, continuous Optuna
rf:
  n_estimators_grid: [100, 300, 500]                 # reduced from 4 to 3 values
  max_depth_grid: [null, 20, 40]                     # reduced from 5 to 3 values
  min_samples_split_grid: [2, 10, 20]                # reduced from 4 to 3 values
  min_samples_leaf_grid: [1, 4, 8]                   # reduced from 4 to 3 values
  max_features_grid: ["sqrt", 0.5, 0.7]              # reduced from 5 to 3 values
  class_weight_options: "balanced"
  optuna_n_estimators: [100, 800]
  optuna_max_depth: [5, 50]
  optuna_min_samples_split: [2, 30]
  optuna_min_samples_leaf: [1, 15]
  optuna_max_features: [0.05, 0.8]
  n_iter: 2

# XGBoost: pruned grid, continuous Optuna
xgboost:
  n_estimators_grid: [100, 400, 800]                 # reduced from 5 to 3 values
  max_depth_grid: [4, 6, 8]                          # reduced from 6 to 3 values
  learning_rate_grid: [0.03, 0.1, 0.2]               # reduced from 5 to 3 values
  subsample_grid: [0.7, 0.85, 1.0]                   # reduced from 4 to 3 values
  colsample_bytree_grid: [0.5, 0.7, 1.0]             # reduced from 5 to 3 values
  scale_pos_weight_grid: [5.0, 6.0, 7.0]             # reduced from 5 to 3 values
  min_child_weight_grid: [2, 6, 10]                  # reduced from 5 to 3 values
  gamma_grid: [0.0, 0.5, 2.0]                        # reduced from 6 to 3 values
  reg_alpha_grid: [0.001, 0.01, 1.0]                 # reduced from 5 to 3 values
  reg_lambda_grid: [0.1, 5.0, 50.0]                  # reduced from 6 to 3 values
  optuna_n_estimators: [50, 1500]
  optuna_max_depth: [3, 10]
  optuna_learning_rate: [0.005, 0.3]
  optuna_min_child_weight: [0.1, 20.0]
  optuna_gamma: [0.0, 2.0]
  optuna_subsample: [0.5, 1.0]
  optuna_colsample_bytree: [0.3, 1.0]
  optuna_reg_alpha: [1.0e-8, 10.0]
  optuna_reg_lambda: [1.0e-2, 50.0]
  tree_method: hist
  n_iter: 2

# Calibration: OOF-posthoc for unbiased estimates
calibration:
  enabled: true
  strategy: oof_posthoc
  method: isotonic
  cv: 5

# Thresholding
thresholds:
  objective: fixed_spec
  fixed_spec: 0.95
  fbeta: 1.0
  fixed_ppv: 0.10
  threshold_source: val
  target_prevalence_source: test
  target_prevalence_fixed: null
  risk_prob_source: test

# Evaluation: reduced overhead for 1-hour constraint
evaluation:
  test_ci_bootstrap: true
  n_boot: 100                                        # down from 200 (adequate for ranking)
  boot_random_state: 0
  learning_curve: false                              # disabled for speed
  lc_train_sizes: [0.1, 0.25, 0.5, 0.75, 1.0]
  feature_reports: true
  feature_report_max: 200
  control_spec_targets: [0.90, 0.95, 0.99]
  toprisk_fracs: [0.01, 0.05, 0.10]

# DCA
dca:
  compute_dca: true
  dca_threshold_min: 0.0005
  dca_threshold_max: 1.0
  dca_threshold_step: 0.001
  dca_report_points: [0.01, 0.05, 0.10, 0.20]

# Outputs
output:
  save_train_preds: true
  save_train_oof: true
  save_val_preds: true
  save_test_preds: true
  save_calibration: true
  calib_bins: 6
  save_controls_oof: true
  save_feature_importance: true
  feature_reports: true
  plot_format: png
  plot_dpi: 300
  save_plots: true
  plot_roc: true
  plot_pr: true
  plot_calibration: true
  plot_risk_distribution: true
  plot_dca: true
  plot_learning_curve: false
  plot_oof_combined: true
  plot_optuna: true
