===============================================================================
CELIAC RISK ML - CONFIGURATION KNOBS CHEATSHEET
===============================================================================

Reference for tuning ced-ml pipeline via YAML config files.
See: configs/splits_config.yaml, configs/training_config.yaml

===============================================================================
SPLITS CONFIGURATION (splits_config.yaml)
===============================================================================

MODE:
  mode: development     Default. TRAIN/VAL/TEST only.
  mode: holdout         Creates 30% holdout for final external validation.
                        Use ONLY for final reporting.

SCENARIOS (case type inclusion):
  scenarios: [IncidentOnly]              Prospective prediction only.
                                         Train on incident, test on incident.

  scenarios: [PrevalentOnly]             Prevalent cases only.
                                         Train on prevalent, test on prevalent.

  scenarios: [IncidentPlusPrevalent]     Both case types.
                                         Train on both (or incident + sampled prevalent),
                                         test on incident only (if prevalent_train_only=true).

  Note: Can specify multiple scenarios to generate splits for each.
        Example: scenarios: [IncidentOnly, IncidentPlusPrevalent]

REPEATED SPLITS (for confidence intervals):
  n_splits: 1           Single split (fast iteration)
  n_splits: 10          10 repeated splits (robust CIs)
  seed_start: 0         Seeds 0 through (seed_start + n_splits - 1)

SPLIT SIZES:
  val_size: 0.25        25% of working set for validation
  test_size: 0.25       25% of working set for test
  holdout_size: 0.30    30% reserved when mode=holdout

PREVALENT CASE HANDLING:
  prevalent_train_only: true    Restrict prevalent to TRAIN (no VAL/TEST leak)
  prevalent_train_frac: 0.5     Sample 50% of prevalent cases

CONTROL DOWNSAMPLING:
  train_control_per_case: 5     5 controls per case in TRAIN (addresses imbalance)
  eval_control_per_case: null   Use all controls in VAL/TEST (realistic prevalence)

===============================================================================
TRAINING CONFIGURATION (training_config.yaml)
===============================================================================

CROSS-VALIDATION (Outer CV):
  folds: 5              Outer CV folds per repeat
  repeats: 10           Repeats of outer CV
                        Total OOF predictions = folds x repeats per sample
                        Production: 5x10 = 50 OOF predictions

HYPERPARAMETER SEARCH (Inner CV):
  inner_folds: 5        Folds for inner CV
  n_iter: 200           RandomizedSearchCV iterations
  scoring: roc_auc      Metric for hyperparameter optimization
                        Options:
                          roc_auc             - Area under ROC curve (discrimination)
                          average_precision   - Area under PR curve (precision-recall)
                          neg_brier_score     - Calibration quality (negative Brier score)
                        Production: neg_brier_score (optimizes calibration)

OPTUNA HYPERPARAMETER OPTIMIZATION (Alternative to RandomizedSearchCV):
  enabled: false        Enable Optuna (vs RandomizedSearchCV)
  n_trials: 100         Number of hyperparameter trials
  sampler: tpe          Sampling algorithm
                        Options:
                          tpe       - Tree-structured Parzen Estimator (recommended)
                          random    - Random search (baseline)
                          cmaes     - CMA-ES (continuous params only)
                          grid      - Grid search (small spaces)
  sampler_seed: 42      RNG seed for reproducibility
  pruner: median        Early stopping strategy
                        Options:
                          median    - Stop if below median (2-3x speedup)
                          percentile - Stop if below 25th percentile (aggressive)
                          hyperband - Successive halving (max efficiency)
                          none      - No pruning (like RandomizedSearchCV)
  pruner_n_startup_trials: 5   Trials before pruning starts
  save_trials_csv: true        Export trials to CSV
  storage: null         Optional: sqlite:///study.db for persistence

  Performance: 2-5x faster than RandomizedSearchCV for expensive models (XGB, RF)
  See: docs/adr/ADR-018-optuna-hyperparameter-optimization.md

FEATURE SELECTION PIPELINE:
  feature_select: hybrid    Options: none, kbest, l1_stability, hybrid
  screen_method: mannwhitney    Pre-screening: mannwhitney or f_classif
  screen_top_n: 1000            2920 proteins -> 1000 after screening
  k_grid: [25,50,100,200,300,400,600,800]   K-best values to tune
  stability_thresh: 0.75    Keep proteins selected in >= 75% of folds
  stable_corr_thresh: 0.85  Remove proteins with |r| > 0.85

THRESHOLD SELECTION:
  objective: fixed_spec     Options: max_f1, max_fbeta, youden, fixed_spec, fixed_ppv
  fixed_spec: 0.95          Target 95% specificity (5% FPR)
  threshold_source: val     Select threshold on VAL (prevents test leakage)
  target_prevalence_source: test    Use TEST prevalence for calibration

EVALUATION:
  test_ci_bootstrap: true   Compute bootstrap confidence intervals
  n_boot: 500               Bootstrap iterations
  learning_curve: true      Generate learning curves
  lc_cv: 3                  CV folds for learning curves

DCA (Decision Curve Analysis):
  compute_dca: true
  dca_threshold_min: 0.0005     0.05% threshold minimum
  dca_threshold_max: 1.0        100% threshold maximum
  dca_threshold_step: 0.001     0.1% steps

OUTPUT:
  save_calibration: true
  calib_bins: 6
  feature_reports: true
  feature_report_max: 200

===============================================================================
CLI USAGE & OVERRIDES
===============================================================================

BASIC COMMAND:
  python -m ced_ml.cli.main train --config configs/training_config.yaml

CONFIG OVERRIDES (modify config without editing YAML):
  --override path.to.param=value

  Examples:
    # Enable Optuna
    --override optuna.enabled=true --override optuna.n_trials=200

    # Quick test run
    --override cv.repeats=3 --override cv.folds=2 --override cv.n_iter=10

    # Change scoring metric
    --override cv.scoring=average_precision

    # Adjust feature selection
    --override features.screen_top_n=500 --override features.stability_thresh=0.80

COMBINING MULTIPLE OVERRIDES:
  python -m ced_ml.cli.main train \
    --config configs/training_config.yaml \
    --override optuna.enabled=true \
    --override optuna.n_trials=150 \
    --override optuna.sampler=tpe \
    --override cv.repeats=5

===============================================================================
WORKFLOW RECIPES
===============================================================================

1. DEVELOPMENT (fast iteration):
   # Option A: Edit config file
   # training_config.yaml
   cv:
     folds: 2
     repeats: 3
     n_iter: 3
     inner_folds: 2

   python -m ced_ml.cli.main train --config configs/training_config.yaml

   # Option B: Use CLI overrides
   python -m ced_ml.cli.main train \
     --config configs/training_config.yaml \
     --override cv.folds=2 \
     --override cv.repeats=3 \
     --override cv.n_iter=3

2. QUALITY TEST (single split, thorough CV):
   # training_config.yaml
   cv:
     folds: 5
     repeats: 10
     n_iter: 200
     inner_folds: 5
     scoring: neg_brier_score

   python -m ced_ml.cli.main train --config configs/training_config.yaml

3. OPTUNA OPTIMIZATION (faster hyperparameter search):
   # Option A: Edit config file
   optuna:
     enabled: true
     n_trials: 200
     sampler: tpe
     pruner: median

   # Option B: CLI override
   python -m ced_ml.cli.main train \
     --config configs/training_config.yaml \
     --override optuna.enabled=true \
     --override optuna.n_trials=200 \
     --override optuna.sampler=tpe \
     --override optuna.pruner=median

   # Compare: RandomizedSearchCV vs Optuna
   # RandomizedSearchCV: 200 iters × 5 folds = 1000 fits (~45 min for XGB)
   # Optuna (TPE+median): 100 trials × ~2.5 folds = 250 fits (~12 min, better params)

4. PRODUCTION (robust CIs across repeated splits):
   # splits_config.yaml
   n_splits: 10
   seed_start: 0

   # training_config.yaml (same as Quality Test)

   # HPC: Submit via job array
   # See: run_hpc.sh and configs/pipeline_hpc.yaml

5. HOLDOUT VALIDATION (final external validation):
   # splits_config.yaml
   mode: holdout
   holdout_size: 0.30

   # Run ONCE only - no parameter tuning allowed
   python -m ced_ml.cli.main eval-holdout \
     --config configs/training_config.yaml \
     --holdout-idx splits/.../HOLDOUT_idx.csv

===============================================================================
MODEL-SPECIFIC NOTES
===============================================================================

LOGISTIC REGRESSION (LR_EN):
  C range: 1e-4 to 1e1 (20 log-spaced values)
  L1 ratio: 0.01 to 0.60 (mix of L1/L2)
  Best for: interpretable coefficients, probability calibration

RANDOM FOREST (RF):
  max_depth: [8, 10, 12, 14]
  min_samples_leaf: [2, 4, 6]
  Memory: 64GB recommended (high memory for permutation importance)
  Best for: nonlinear patterns, feature importance, robust to outliers

XGBOOST:
  max_depth: [3, 7] (shallow for high-dim data)
  learning_rate: [0.01, 0.1]
  scale_pos_weight: auto (~295 for Celiac imbalance)
  Best for: predictive power, nonlinear interactions

LINEAR SVM (LinSVM_cal):
  C range: 1e-4 to 1e4 (10 log-spaced values)
  Calibration: sigmoid (Platt scaling)
  Best for: high-dimensional data, margin-based separation

===============================================================================
HPC RESOURCE GUIDELINES
===============================================================================

Model    CPUs    Memory (total)    Notes
-----    ----    --------------    -----
RF       8       64GB              Need high-mem nodes
XGB      8       32GB              Histogram method, moderate
SVM      8       16GB              Lightweight
LR       8       16GB              Lightweight

===============================================================================
MONITORING & DEBUGGING
===============================================================================

Job status:
  bjobs -w | grep CeD_

Check completed models:
  ls results_*/*/core/test_metrics.csv

Postprocessing:
  ced postprocess --results-dir results_hpc --n-boot 500

Visualization:
  Rscript compare_models_faith.R --results_root results_hpc

===============================================================================
KEY FILES
===============================================================================

configs/splits_config.yaml      Split configuration
configs/training_config.yaml    Training configuration
docs/txt/PARAMETERS_REFERENCE.txt   Full parameter reference
docs/ARCHITECTURE.md            Algorithm documentation

===============================================================================
PRODUCTION DEFAULTS (publication-ready)
===============================================================================

# splits_config.yaml
mode: development
n_splits: 10
seed_start: 0

# training_config.yaml
cv:
  folds: 5
  repeats: 10
  scoring: neg_brier_score
  n_iter: 200
  inner_folds: 5

features:
  feature_select: hybrid
  screen_top_n: 1000
  stability_thresh: 0.75

thresholds:
  objective: fixed_spec
  fixed_spec: 0.95

evaluation:
  test_ci_bootstrap: true
  n_boot: 500

===============================================================================
