===============================================================================
CELIAC RISK ML - CONFIGURATION KNOBS CHEATSHEET
===============================================================================

Reference for tuning ced-ml pipeline via YAML config files.
See: configs/splits_config.yaml, configs/training_config.yaml

===============================================================================
SPLITS CONFIGURATION (splits_config.yaml)
===============================================================================

MODE:
  mode: development     Default. TRAIN/VAL/TEST only.
  mode: holdout         Creates 30% holdout for final external validation.
                        Use ONLY for final reporting.

SCENARIOS (case type inclusion):
  scenarios: [IncidentOnly]              Prospective prediction only.
                                         Train on incident, test on incident.

  scenarios: [PrevalentOnly]             Prevalent cases only.
                                         Train on prevalent, test on prevalent.

  scenarios: [IncidentPlusPrevalent]     Both case types.
                                         Train on both (or incident + sampled prevalent),
                                         test on incident only (if prevalent_train_only=true).

  Note: Can specify multiple scenarios to generate splits for each.
        Example: scenarios: [IncidentOnly, IncidentPlusPrevalent]

REPEATED SPLITS (for confidence intervals):
  n_splits: 1           Single split (fast iteration)
  n_splits: 10          10 repeated splits (robust CIs)
  seed_start: 0         Seeds 0 through (seed_start + n_splits - 1)

SPLIT SIZES:
  val_size: 0.25        25% of working set for validation
  test_size: 0.25       25% of working set for test
  holdout_size: 0.30    30% reserved when mode=holdout

PREVALENT CASE HANDLING:
  prevalent_train_only: true    Restrict prevalent to TRAIN (no VAL/TEST leak)
  prevalent_train_frac: 0.5     Sample 50% of prevalent cases

CONTROL DOWNSAMPLING:
  train_control_per_case: 5     5 controls per case in TRAIN (addresses imbalance)
  eval_control_per_case: null   Use all controls in VAL/TEST (realistic prevalence)

===============================================================================
TRAINING CONFIGURATION (training_config.yaml)
===============================================================================

CROSS-VALIDATION (Outer CV):
  folds: 5              Outer CV folds per repeat
  repeats: 10           Repeats of outer CV
                        Total OOF predictions = folds x repeats per sample
                        Production: 5x10 = 50 OOF predictions

HYPERPARAMETER SEARCH (Inner CV):
  inner_folds: 5        Folds for inner CV
  n_iter: 200           RandomizedSearchCV iterations
  scoring: roc_auc      Metric for hyperparameter optimization
                        Options:
                          roc_auc             - Area under ROC curve (discrimination)
                          average_precision   - Area under PR curve (precision-recall)
                          neg_brier_score     - Calibration quality (negative Brier score)
                        Production: neg_brier_score (optimizes calibration)

FEATURE SELECTION PIPELINE:
  feature_select: hybrid    Options: none, kbest, l1_stability, hybrid
  screen_method: mannwhitney    Pre-screening: mannwhitney or f_classif
  screen_top_n: 1000            2920 proteins -> 1000 after screening
  k_grid: [25,50,100,200,300,400,600,800]   K-best values to tune
  stability_thresh: 0.75    Keep proteins selected in >= 75% of folds
  stable_corr_thresh: 0.85  Remove proteins with |r| > 0.85

THRESHOLD SELECTION:
  objective: fixed_spec     Options: max_f1, max_fbeta, youden, fixed_spec, fixed_ppv
  fixed_spec: 0.95          Target 95% specificity (5% FPR)
  threshold_source: val     Select threshold on VAL (prevents test leakage)
  target_prevalence_source: test    Use TEST prevalence for calibration

EVALUATION:
  test_ci_bootstrap: true   Compute bootstrap confidence intervals
  n_boot: 500               Bootstrap iterations
  learning_curve: true      Generate learning curves
  lc_cv: 3                  CV folds for learning curves

DCA (Decision Curve Analysis):
  compute_dca: true
  dca_threshold_min: 0.0005     0.05% threshold minimum
  dca_threshold_max: 1.0        100% threshold maximum
  dca_threshold_step: 0.001     0.1% steps

OUTPUT:
  save_calibration: true
  calib_bins: 6
  feature_reports: true
  feature_report_max: 200

===============================================================================
WORKFLOW RECIPES
===============================================================================

1. DEVELOPMENT (fast iteration):
   # splits_config.yaml
   n_splits: 1

   # training_config.yaml
   cv:
     folds: 2
     repeats: 3
     n_iter: 3
     inner_folds: 2

   ced train --config configs/training_config.yaml --model LR_EN ...

2. QUALITY TEST (single split, thorough CV):
   # splits_config.yaml
   n_splits: 1

   # training_config.yaml
   cv:
     folds: 5
     repeats: 10
     n_iter: 200
     inner_folds: 5
     scoring: neg_brier_score

   ced train --model LR_EN,RF,XGBoost,LinSVM_cal ...

3. PRODUCTION (robust CIs across repeated splits):
   # splits_config.yaml
   n_splits: 10
   seed_start: 0

   # training_config.yaml (same as Quality Test)

   # Submit all 4 models x 10 seeds = 40 jobs
   bsub < CeD_production.lsf

4. HOLDOUT VALIDATION (final external validation):
   # splits_config.yaml
   mode: holdout
   holdout_size: 0.30

   # Run ONCE only - no parameter tuning allowed
   ced eval-holdout --holdout-idx splits/.../HOLDOUT_idx.csv ...

===============================================================================
MODEL-SPECIFIC NOTES
===============================================================================

LOGISTIC REGRESSION (LR_EN):
  C range: 1e-4 to 1e1 (20 log-spaced values)
  L1 ratio: 0.01 to 0.60 (mix of L1/L2)
  Best for: interpretable coefficients, probability calibration

RANDOM FOREST (RF):
  max_depth: [8, 10, 12, 14]
  min_samples_leaf: [2, 4, 6]
  Memory: 64GB recommended (high memory for permutation importance)
  Best for: nonlinear patterns, feature importance, robust to outliers

XGBOOST:
  max_depth: [3, 7] (shallow for high-dim data)
  learning_rate: [0.01, 0.1]
  scale_pos_weight: auto (~295 for Celiac imbalance)
  Best for: predictive power, nonlinear interactions

LINEAR SVM (LinSVM_cal):
  C range: 1e-4 to 1e4 (10 log-spaced values)
  Calibration: sigmoid (Platt scaling)
  Best for: high-dimensional data, margin-based separation

===============================================================================
HPC RESOURCE GUIDELINES
===============================================================================

Model    CPUs    Memory (total)    Notes
-----    ----    --------------    -----
RF       8       64GB              Need high-mem nodes
XGB      8       32GB              Histogram method, moderate
SVM      8       16GB              Lightweight
LR       8       16GB              Lightweight

===============================================================================
MONITORING & DEBUGGING
===============================================================================

Job status:
  bjobs -w | grep CeD_

Check completed models:
  ls results_*/*/core/test_metrics.csv

Postprocessing:
  ced postprocess --results-dir results_production --n-boot 500

Visualization:
  Rscript compare_models_faith.R --results_root results_production

===============================================================================
KEY FILES
===============================================================================

configs/splits_config.yaml      Split configuration
configs/training_config.yaml    Training configuration
docs/txt/PARAMETERS_REFERENCE.txt   Full parameter reference
docs/pipeline.md                Algorithm documentation

===============================================================================
PRODUCTION DEFAULTS (publication-ready)
===============================================================================

# splits_config.yaml
mode: development
n_splits: 10
seed_start: 0

# training_config.yaml
cv:
  folds: 5
  repeats: 10
  scoring: neg_brier_score
  n_iter: 200
  inner_folds: 5

features:
  feature_select: hybrid
  screen_top_n: 1000
  stability_thresh: 0.75

thresholds:
  objective: fixed_spec
  fixed_spec: 0.95

evaluation:
  test_ci_bootstrap: true
  n_boot: 500

===============================================================================
